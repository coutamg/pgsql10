PG缓冲区源码--文件介绍

buf_init.c
Buf初始化功能，初始化出三处内存空间，分别被BufferDescriptors、BufferBlocks、
SharedBufHash、StrategyControl描述，注意这四个变量，他们表述了数据缓冲区的
基本结构。Buf 的整体结构在 src/backend/storage/buffer/buf_init.c 中有
InitBufferPool 函数

buf_table.c
Buf管理的辅助文件，完成对SharedBufHash变量（此变量便于对缓冲区中的缓存块进行查
找使用）表示的内存的管理操作（如初始化、查找、插入、删除等），主要有如下函数调用：
       SharedBufHash
       InitBufTable
       BufTableHashCode
       BufTableLookup
       BufTableInsert
       BufTableDelete

bufmgr.c
Buf的管理文件，完成对buf的管理操作，如buf的分配、回收等。主要的一些函数如 
ReadBuffer、ReadBufferExtended 、ReleaseBuffer、MarkBufferDirty 等

freelist.c
Buf 替换策略相关代码，完成对缓冲区替换策略的管理，主要有函数AddBufferToRing、
FreeAccessStrategy、GetAccessStrategy、GetBufferFromRing、StrategyFreeBuffer、
StrategyGetBuffer、StrategyInitialize、StrategyRejectBuffer、StrategyShmemSize、
StrategySyncStart

localbuf.c
本地缓存管理。本地缓存，指的是对临时表的管理（PG有临时表的概念，create temp table，这些
临时表被创建即进入内存，实则进入缓存）



src/backend/storage/buffer/README

Notes About Shared Buffer Access Rules
======================================

There are two separate access control mechanisms for shared disk buffers:
reference counts (a/k/a pin counts) and buffer content locks.  (Actually,
there's a third level of access control: one must hold the appropriate kind
of lock on a relation before one can legally access any page belonging to
the relation.  Relation-level locks are not discussed here.)

Pins: one must "hold a pin on" a buffer (increment its reference count)
before being allowed to do anything at all with it.  An unpinned buffer is
subject to being reclaimed and reused for a different page at any instant,
so touching it is unsafe.  Normally a pin is acquired via ReadBuffer and
released via ReleaseBuffer.  It is OK and indeed common for a single
backend to pin a page more than once concurrently; the buffer manager
handles this efficiently.  It is considered OK to hold a pin for long
intervals --- for example, sequential scans hold a pin on the current page
until done processing all the tuples on the page, which could be quite a
while if the scan is the outer scan of a join.  Similarly, a btree index
scan may hold a pin on the current index page.  This is OK because normal
operations never wait for a page's pin count to drop to zero.  (Anything
that might need to do such a wait is instead handled by waiting to obtain
the relation-level lock, which is why you'd better hold one first.)  Pins
may not be held across transaction boundaries, however.

Buffer content locks: there are two kinds of buffer lock, shared and exclusive,
which act just as you'd expect: multiple backends can hold shared locks on
the same buffer, but an exclusive lock prevents anyone else from holding
either shared or exclusive lock.  (These can alternatively be called READ
and WRITE locks.)  These locks are intended to be short-term: they should not
be held for long.  Buffer locks are acquired and released by LockBuffer().
It will *not* work for a single backend to try to acquire multiple locks on
the same buffer.  One must pin a buffer before trying to lock it.

Buffer access rules:

1. To scan a page for tuples, one must hold a pin and either shared or
exclusive content lock.  To examine the commit status (XIDs and status bits)
of a tuple in a shared buffer, one must likewise hold a pin and either shared
or exclusive lock.

2. Once one has determined that a tuple is interesting (visible to the
current transaction) one may drop the content lock, yet continue to access
the tuple's data for as long as one holds the buffer pin.  This is what is
typically done by heap scans, since the tuple returned by heap_fetch
contains a pointer to tuple data in the shared buffer.  Therefore the
tuple cannot go away while the pin is held (see rule #5).  Its state could
change, but that is assumed not to matter after the initial determination
of visibility is made.

3. To add a tuple or change the xmin/xmax fields of an existing tuple,
one must hold a pin and an exclusive content lock on the containing buffer.
This ensures that no one else might see a partially-updated state of the
tuple while they are doing visibility checks.

4. It is considered OK to update tuple commit status bits (ie, OR the
values HEAP_XMIN_COMMITTED, HEAP_XMIN_INVALID, HEAP_XMAX_COMMITTED, or
HEAP_XMAX_INVALID into t_infomask) while holding only a shared lock and
pin on a buffer.  This is OK because another backend looking at the tuple
at about the same time would OR the same bits into the field, so there
is little or no risk of conflicting update; what's more, if there did
manage to be a conflict it would merely mean that one bit-update would
be lost and need to be done again later.  These four bits are only hints
(they cache the results of transaction status lookups in pg_xact), so no
great harm is done if they get reset to zero by conflicting updates.
Note, however, that a tuple is frozen by setting both HEAP_XMIN_INVALID
and HEAP_XMIN_COMMITTED; this is a critical update and accordingly requires
an exclusive buffer lock (and it must also be WAL-logged).

5. To physically remove a tuple or compact free space on a page, one
must hold a pin and an exclusive lock, *and* observe while holding the
exclusive lock that the buffer's shared reference count is one (ie,
no other backend holds a pin).  If these conditions are met then no other
backend can perform a page scan until the exclusive lock is dropped, and
no other backend can be holding a reference to an existing tuple that it
might expect to examine again.  Note that another backend might pin the
buffer (increment the refcount) while one is performing the cleanup, but
it won't be able to actually examine the page until it acquires shared
or exclusive content lock.


Obtaining the lock needed under rule #5 is done by the bufmgr routines
LockBufferForCleanup() or ConditionalLockBufferForCleanup().  They first get
an exclusive lock and then check to see if the shared pin count is currently
1.  If not, ConditionalLockBufferForCleanup() releases the exclusive lock and
then returns false, while LockBufferForCleanup() releases the exclusive lock
(but not the caller's pin) and waits until signaled by another backend,
whereupon it tries again.  The signal will occur when UnpinBuffer decrements
the shared pin count to 1.  As indicated above, this operation might have to
wait a good while before it acquires the lock, but that shouldn't matter much
for concurrent VACUUM.  The current implementation only supports a single
waiter for pin-count-1 on any particular shared buffer.  This is enough for
VACUUM's use, since we don't allow multiple VACUUMs concurrently on a single
relation anyway.  Anyone wishing to obtain a cleanup lock outside of recovery
or a VACUUM must use the conditional variant of the function.


Buffer Manager's Internal Locking
---------------------------------

Before PostgreSQL 8.1, all operations of the shared buffer manager itself
were protected by a single system-wide lock, the BufMgrLock, which
unsurprisingly proved to be a source of contention.  The new locking scheme
avoids grabbing system-wide exclusive locks in common code paths.  It works
like this:

* There is a system-wide LWLock, the BufMappingLock, that notionally
protects the mapping from buffer tags (page identifiers) to buffers.
(Physically, it can be thought of as protecting the hash table maintained
by buf_table.c.)  To look up whether a buffer exists for a tag, it is
sufficient to obtain share lock on the BufMappingLock.  Note that one
must pin the found buffer, if any, before releasing the BufMappingLock.
To alter the page assignment of any buffer, one must hold exclusive lock
on the BufMappingLock.  This lock must be held across adjusting the buffer's
header fields and changing the buf_table hash table.  The only common
operation that needs exclusive lock is reading in a page that was not
in shared buffers already, which will require at least a kernel call
and usually a wait for I/O, so it will be slow anyway.

* As of PG 8.2, the BufMappingLock has been split into NUM_BUFFER_PARTITIONS
separate locks, each guarding a portion of the buffer tag space.  This allows
further reduction of contention in the normal code paths.  The partition
that a particular buffer tag belongs to is determined from the low-order
bits of the tag's hash value.  The rules stated above apply to each partition
independently.  If it is necessary to lock more than one partition at a time,
they must be locked in partition-number order to avoid risk of deadlock.

* A separate system-wide spinlock, buffer_strategy_lock, provides mutual
exclusion for operations that access the buffer free list or select
buffers for replacement.  A spinlock is used here rather than a lightweight
lock for efficiency; no other locks of any sort should be acquired while
buffer_strategy_lock is held.  This is essential to allow buffer replacement
to happen in multiple backends with reasonable concurrency.

* Each buffer header contains a spinlock that must be taken when examining
or changing fields of that buffer header.  This allows operations such as
ReleaseBuffer to make local state changes without taking any system-wide
lock.  We use a spinlock, not an LWLock, since there are no cases where
the lock needs to be held for more than a few instructions.

Note that a buffer header's spinlock does not control access to the data
held within the buffer.  Each buffer header also contains an LWLock, the
"buffer content lock", that *does* represent the right to access the data
in the buffer.  It is used per the rules above.

There is yet another set of per-buffer LWLocks, the io_in_progress locks,
that are used to wait for I/O on a buffer to complete.  The process doing
a read or write takes exclusive lock for the duration, and processes that
need to wait for completion try to take shared locks (which they release
immediately upon obtaining).  XXX on systems where an LWLock represents
nontrivial resources, it's fairly annoying to need so many locks.  Possibly
we could use per-backend LWLocks instead (a buffer header would then contain
a field to show which backend is doing its I/O).


Normal Buffer Replacement Strategy
----------------------------------

There is a "free list" of buffers that are prime candidates for replacement.
In particular, buffers that are completely free (contain no valid page) are
always in this list.  We could also throw buffers into this list if we
consider their pages unlikely to be needed soon; however, the current
algorithm never does that.  The list is singly-linked using fields in the
buffer headers; we maintain head and tail pointers in global variables.
(Note: although the list links are in the buffer headers, they are
considered to be protected by the buffer_strategy_lock, not the buffer-header
spinlocks.)  To choose a victim buffer to recycle when there are no free
buffers available, we use a simple clock-sweep algorithm, which avoids the
need to take system-wide locks during common operations.  It works like
this:

Each buffer header contains a usage counter, which is incremented (up to a
small limit value) whenever the buffer is pinned.  (This requires only the
buffer header spinlock, which would have to be taken anyway to increment the
buffer reference count, so it's nearly free.)

The "clock hand" is a buffer index, nextVictimBuffer, that moves circularly
through all the available buffers.  nextVictimBuffer is protected by the
buffer_strategy_lock.

The algorithm for a process that needs to obtain a victim buffer is:

1. Obtain buffer_strategy_lock.

2. If buffer free list is nonempty, remove its head buffer.  Release
buffer_strategy_lock.  If the buffer is pinned or has a nonzero usage count,
it cannot be used; ignore it go back to step 1.  Otherwise, pin the buffer,
and return it.

3. Otherwise, the buffer free list is empty.  Select the buffer pointed to by
nextVictimBuffer, and circularly advance nextVictimBuffer for next time.
Release buffer_strategy_lock.

4. If the selected buffer is pinned or has a nonzero usage count, it cannot
be used.  Decrement its usage count (if nonzero), reacquire
buffer_strategy_lock, and return to step 3 to examine the next buffer.

5. Pin the selected buffer, and return.

(Note that if the selected buffer is dirty, we will have to write it out
before we can recycle it; if someone else pins the buffer meanwhile we will
have to give up and try another buffer.  This however is not a concern
of the basic select-a-victim-buffer algorithm.)


Buffer Ring Replacement Strategy
---------------------------------

When running a query that needs to access a large number of pages just once,
such as VACUUM or a large sequential scan, a different strategy is used.
A page that has been touched only by such a scan is unlikely to be needed
again soon, so instead of running the normal clock sweep algorithm and
blowing out the entire buffer cache, a small ring of buffers is allocated
using the normal clock sweep algorithm and those buffers are reused for the
whole scan.  This also implies that much of the write traffic caused by such
a statement will be done by the backend itself and not pushed off onto other
processes.

For sequential scans, a 256KB ring is used. That's small enough to fit in L2
cache, which makes transferring pages from OS cache to shared buffer cache
efficient.  Even less would often be enough, but the ring must be big enough
to accommodate all pages in the scan that are pinned concurrently.  256KB
should also be enough to leave a small cache trail for other backends to
join in a synchronized seq scan.  If a ring buffer is dirtied and its LSN
updated, we would normally have to write and flush WAL before we could
re-use the buffer; in this case we instead discard the buffer from the ring
and (later) choose a replacement using the normal clock-sweep algorithm.
Hence this strategy works best for scans that are read-only (or at worst
update hint bits).  In a scan that modifies every page in the scan, like a
bulk UPDATE or DELETE, the buffers in the ring will always be dirtied and
the ring strategy effectively degrades to the normal strategy.

VACUUM uses a 256KB ring like sequential scans, but dirty pages are not
removed from the ring.  Instead, WAL is flushed if needed to allow reuse of
the buffers.  Before introducing the buffer ring strategy in 8.3, VACUUM's
buffers were sent to the freelist, which was effectively a buffer ring of 1
buffer, resulting in excessive WAL flushing.  Allowing VACUUM to update
256KB between WAL flushes should be more efficient.

Bulk writes work similarly to VACUUM.  Currently this applies only to
COPY IN and CREATE TABLE AS SELECT.  (Might it be interesting to make
seqscan UPDATE and DELETE use the bulkwrite strategy?)  For bulk writes
we use a ring size of 16MB (but not more than 1/8th of shared_buffers).
Smaller sizes have been shown to result in the COPY blocking too often
for WAL flushes.  While it's okay for a background vacuum to be slowed by
doing its own WAL flushing, we'd prefer that COPY not be subject to that,
so we let it use up a bit more of the buffer arena.


Background Writer's Processing
------------------------------

The background writer is designed to write out pages that are likely to be
recycled soon, thereby offloading the writing work from active backends.
To do this, it scans forward circularly from the current position of
nextVictimBuffer (which it does not change!), looking for buffers that are
dirty and not pinned nor marked with a positive usage count.  It pins,
writes, and releases any such buffer.

If we can assume that reading nextVictimBuffer is an atomic action, then
the writer doesn't even need to take buffer_strategy_lock in order to look
for buffers to write; it needs only to spinlock each buffer header for long
enough to check the dirtybit.  Even without that assumption, the writer
only needs to take the lock long enough to read the variable value, not
while scanning the buffers.  (This is a very substantial improvement in
the contention cost of the writer compared to PG 8.0.)

The background writer takes shared content lock on a buffer while writing it
out (and anyone else who flushes buffer contents to disk must do so too).
This ensures that the page image transferred to disk is reasonably consistent.
We might miss a hint-bit update or two but that isn't a problem, for the same
reasons mentioned under buffer access rules.

As of 8.4, background writer starts during recovery mode when there is
some form of potentially extended recovery to perform. It performs an
identical service to normal processing, except that checkpoints it
writes are technically restartpoints.


关于共享缓冲区访问规则的说明

共享磁盘缓冲区有两种独立的访问控制机制：引用计数（a/k/a pin 计数）和缓冲区内容锁。（实际上，存在第三级访问控制：必须在关系上持有适当类型的锁，然后才能合法地访问属于该关系的任何页面。这里不讨论关系级锁。）

引脚：在被允许对它做任何事情之前，必须“在”缓冲区上“保持一个引脚”（增加它的引用计数）。未固定的缓冲区随时可能被回收并重新用于不同的页面，因此触摸它是不安全的。通常一个 pin 是通过 ReadBuffer 获取并通过 ReleaseBuffer 释放的。单个后端同时固定页面不止一次是可以的，而且确实很常见；缓冲区管理器有效地处理这个问题。长时间保持一个 pin 被认为是可以的 --- 例如，顺序扫描在当前页面上保持一个 pin，直到处理完页面上的所有元组，如果扫描是外部扫描，这可能会很长一段时间加入。类似地，btree 索引扫描可能会在当前索引页上保存一个 pin。这没关系，因为正常操作从不等待页面' s 引脚数降至零。（任何可能需要进行这种等待的事情都是通过等待获得关系级锁来处理的，这就是为什么你最好先持有一个。）但是，引脚可能不会跨越事务边界持有。

缓冲区内容锁：有两种类型的缓冲区锁，共享锁和排他锁，它们的作用与您期望的一样：多个后端可以在同一个缓冲区上持有共享锁，但排他锁阻止其他任何人持有共享锁或排他锁. （这些也可以称为 READ 和 WRITE 锁。）这些锁是短期的：它们不应该长期持有。缓冲区锁由 LockBuffer() 获取和释放。单个后端尝试在同一个缓冲区上获取多个锁是行不通的。在尝试锁定它之前，必须固定一个缓冲区。

缓冲区访问规则：

要扫描页面以查找元组，必须持有一个 pin 和共享或独占内容锁。要检查共享缓冲区中元组的提交状态（XID 和状态位），同样必须持有一个 pin 以及共享锁或排他锁。
一旦确定一个元组是有趣的（对当前事务可见），一个人可能会放弃内容锁，但只要持有缓冲区引脚，就可以继续访问元组的数据。这通常由堆扫描完成，因为 heap_fetch 返回的元组包含指向共享缓冲区中元组数据的指针。因此，当 pin 被持有时，元组不能消失（参见规则 #5）。它的状态可能会改变，但在最初确定可见性之后，这被认为无关紧要。
要添加元组或更改现有元组的 xmin/xmax 字段，必须在包含缓冲区上持有一个 pin 和一个排他内容锁。这确保了其他人在进行可见性检查时不会看到元组的部分更新状态。
更新元组提交状态位（即，将值 HEAP_XMIN_COMMITTED、HEAP_XMIN_INVALID、HEAP_XMAX_COMMITTED 或 HEAP_XMAX_INVALID 更新到 t_infomask）被认为是可以的，同时仅在缓冲区上持有共享锁和 pin。这没关系，因为大约在同一时间查看元组的另一个后端会将相同的位 OR 到字段中，因此冲突更新的风险很小或没有；更重要的是，如果确实发生了冲突，那仅意味着一个位更新将丢失并且需要稍后再进行。这四个位只是提示（它们在 pg_xact 中缓存事务状态查找的结果），因此如果它们通过冲突更新重置为零不会造成太大的伤害。但是请注意，通过设置 HEAP_XMIN_INVALID 和 HEAP_XMIN_COMMITTED 来冻结元组；
要物理删除页面上的元组或压缩空闲空间，必须持有一个 pin 和一个独占锁，并在持有独占锁的同时观察缓冲区的共享引用计数为 1（即，没有其他后端持有 pin）。如果满足这些条件，那么在删除独占锁之前，没有其他后端可以执行页面扫描，并且没有其他后端可以持有对可能希望再次检查的现有元组的引用。请注意，另一个后端可能会在执行清理时固定缓冲区（增加引用计数），但它无法实际检查页面，直到它获取共享或独占内容锁。
通过 bufmgr 例程 LockBufferForCleanup() 或 ConditionalLockBufferForCleanup() 获得规则 #5 所需的锁。他们首先获得一个排他锁，然后检查共享 pin 计数当前是否为 1。如果不是，ConditionalLockBufferForCleanup() 释放排他锁，然后返回 false，而 LockBufferForCleanup() 释放排他锁（但不是调用者的 pin）并等待直到另一个后端发出信号，然后再次尝试。当 UnpinBuffer 将共享的 pin 计数减为 1 时，将出现该信号。如上所述，此操作可能需要等待很长时间才能获得锁，但这对于并发 VACUUM 来说并不重要。当前实现仅支持任何特定共享缓冲区上的 pin-count-1 的单个服务员。这对于 VACUUM 的使用来说已经足够了，因为无论如何我们都不允许在单个关系上同时存在多个 VACUUM。任何希望在恢复或 VACUUM 之外获得清理锁的人都必须使用该函数的条件变体。

缓冲区管理器的内部锁定
在 PostgreSQL 8.1 之前，共享缓冲区管理器本身的所有操作都由单个系统范围的锁 BufMgrLock 保护，这毫不奇怪地被证明是争用的来源。新的锁定方案避免了在公共代码路径中获取系统范围的排他锁。它是这样工作的：

有一个系统范围的 LWLock，即 BufMappingLock，它在概念上保护从缓冲区标记（页面标识符）到缓冲区的映射。（物理上，它可以被认为是保护由 buf_table.c 维护的哈希表。）要查找是否存在用于标记的缓冲区，获得 BufMappingLock 上的共享锁就足够了。请注意，必须在释放 BufMappingLock 之前锁定找到的缓冲区（如果有）。要更改任何缓冲区的页面分配，必须在 BufMappingLock 上持有排他锁。在调整缓冲区的头字段和更改 buf_table 哈希表时，必须保持此锁。唯一需要排他锁的常见操作是读取不在共享缓冲区中的页面，这至少需要内核调用并且通常需要等待 I/O，因此无论如何它都会很慢。
从 PG 8.2 开始，BufMappingLock 已被拆分为 NUM_BUFFER_PARTITIONS 个单独的锁，每个锁都保护一部分缓冲区标记空间。这允许进一步减少正常代码路径中的争用。特定缓冲区标签所属的分区由标签哈希值的低位确定。上述规则独立适用于每个分区。如果需要一次锁定多个分区，则必须按分区号顺序锁定它们，以避免死锁风险。
一个单独的系统范围自旋锁 buffer_strategy_lock 为访问缓冲区空闲列表或选择要替换的缓冲区的操作提供互斥。这里使用自旋锁而不是轻量级锁以提高效率；持有 buffer_strategy_lock 时，不应获取任何其他类型的锁。这对于允许在多个后端以合理的并发性进行缓冲区替换至关重要。
每个缓冲区标头都包含一个自旋锁，在检查或更改该缓冲区标头的字段时必须采用该自旋锁。这允许诸如 ReleaseBuffer 之类的操作在不获取任何系统范围的锁的情况下进行本地状态更改。我们使用自旋锁，而不是 LWLock，因为在任何情况下，锁需要保持多于几条指令。
请注意，缓冲区头的自旋锁不控制对缓冲区中保存的数据的访问。每个缓冲区头还包含一个 LWLock，即“缓冲区内容锁”，它确实代表了访问缓冲区中数据的权利。它按照上述规则使用。

还有另一组每个缓冲区的 LWLock，即 io_in_progress 锁，用于等待缓冲区上的 I/O 完成。执行读取或写入的进程在持续时间内获取独占锁，而需要等待完成的进程尝试获取共享锁（它们在获得后立即释放）。XXX 在 LWLock 代表重要资源的系统上，需要这么多锁是相当烦人的。可能我们可以使用 per-backend LWLocks 代替（缓冲区头将包含一个字段来显示哪个后端正在执行其 I/O）。

正常缓冲区替换策略
有一个“空闲列表”的缓冲区是替换的主要候选者。特别是，完全空闲的缓冲区（不包含有效页面）总是在这个列表中。如果我们认为不太可能很快需要它们的页面，我们也可以将缓冲区放入此列表中；但是，当前的算法从不这样做。该列表使用缓冲区标头中的字段单独链接；我们在全局变量中维护头指针和尾指针。（注意：虽然列表链接在缓冲区头中，但它们被认为受 buffer_strategy_lock 保护，而不是缓冲区头自旋锁。）要在没有可用缓冲区可用时选择要回收的牺牲缓冲区，我们使用一个简单的时钟扫描算法，它避免了在常见操作期间需要获取系统范围的锁。它是这样工作的：

每个缓冲区头包含一个使用计数器，每当缓冲区被固定时，该计数器就会增加（直到一个小的限制值）。（这只需要缓冲区头自旋锁，无论如何都必须采用它来增加缓冲区引用计数，因此它几乎是免费的。）

“时钟指针”是一个缓冲区索引 nextVictimBuffer，它在所有可用缓冲区中循环移动。nextVictimBuffer 受 buffer_strategy_lock 保护。

需要获取牺牲缓冲区的进程的算法是：

获取 buffer_strategy_lock。
如果缓冲区空闲列表非空，则删除其头缓冲区。释放 buffer_strategy_lock。如果缓冲区被固定或使用计数非零，则不能使用；忽略它返回步骤 1。否则，固定缓冲区，然后返回它。
否则，缓冲区空闲列表为空。选择nextVictimBuffer指向的缓冲区，循环推进nextVictimBuffer进行下一次。释放 buffer_strategy_lock。
如果所选缓冲区已固定或使用计数非零，则不能使用。减少其使用计数（如果非零），重新获取 buffer_strategy_lock，然后返回步骤 3 以检查下一个缓冲区。
固定选定的缓冲区，然后返回。（注意，如果选择的缓冲区是脏的，我们必须先将其写出，然后才能回收它；如果其他人同时固定缓冲区，我们将不得不放弃并尝试另一个缓冲区。但这不是基本的问题选择受害者缓冲区算法。）
缓冲环更换策略
当运行只需要访问大量页面一次的查询时，例如 VACUUM 或大型顺序扫描，将使用不同的策略。仅被这种扫描触及的页面不太可能很快再次被需要，因此不是运行正常的时钟扫描算法并清空整个缓冲区缓存，而是使用正常的时钟扫描算法分配一小圈缓冲区，并且这些缓冲区被重新用于整个扫描。这也意味着由这样的语句引起的大部分写入流量将由后端本身完成，而不是推到其他进程上。

对于顺序扫描，使用 256KB 的环。这足够小以适合 L2 缓存，这使得将页面从 OS 缓存传输到共享缓冲区缓存有效。甚至更少通常就足够了，但是环必须足够大以容纳扫描中同时固定的所有页面。256KB 也应该足以为其他后端留下一个小的缓存路径来加入同步的 seq 扫描。如果一个环形缓冲区被弄脏并且它的 LSN 更新了，我们通常必须在重新使用缓冲区之前写入和刷新 WAL；在这种情况下，我们改为从环中丢弃缓冲区，并（稍后）使用正常的时钟扫描算法选择替换。因此，此策略最适用于只读扫描（或最坏的更新提示位）。在修改扫描中每个页面的扫描中，如批量更新或删除，

VACUUM 使用类似顺序扫描的 256KB 环，但脏页不会从环中删除。相反，如果需要，WAL 会被刷新以允许重用缓冲区。在 8.3 中引入缓冲区环策略之前，VACUUM 的缓冲区被发送到 freelist，这实际上是 1 个缓冲区的缓冲区环，导致过多的 WAL 刷新。允许 VACUUM 在 WAL 刷新之间更新 256KB 应该更有效。

批量写入与 VACUUM 类似。目前这仅适用于 COPY IN 和 CREATE TABLE AS SELECT。（让 seqscan UPDATE 和 DELETE 使用 bulkwrite 策略会不会很有趣？）对于批量写入，我们使用 16MB 的环大小（但不超过 shared_buffers 的 ⅛th）。较小的尺寸已被证明会导致 WAL 刷新的 COPY 阻塞过于频繁。虽然通过自己的 WAL 刷新来减慢背景真空是可以的，但我们更希望 COPY 不受此影响，因此我们让它占用更多的缓冲区空间。

后台写入器的处理
后台编写器旨在写出可能很快会被回收的页面，从而减轻活动后端的编写工作。为此，它从 nextVictimBuffer 的当前位置（它不会改变！）循环向前扫描，寻找脏的、未固定或标记为正使用计数的缓冲区。它固定、写入和释放任何此类缓冲区。

如果我们可以假设读取 nextVictimBuffer 是一个原子操作，那么编写器甚至不需要使用 buffer_strategy_lock 来查找要写入的缓冲区；它只需要自旋锁定每个缓冲区头足够长的时间来检查脏位。即使没有这个假设，写入器也只需要足够长的时间来读取变量值，而不是在扫描缓冲区时。（与 PG 8.0 相比，这在 writer 的争用成本上是一个非常显着的改进。）

后台写入器在写出缓冲区时对缓冲区进行共享内容锁定（其他任何将缓冲区内容刷新到磁盘的人也必须这样做）。这确保了传输到磁盘的页面图像是合理一致的。我们可能会错过一两次提示位更新，但这不是问题，原因与缓冲区访问规则中提到的相同。

从 8.4 开始，当需要执行某种形式的潜在扩展恢复时，后台写入程序会在恢复模式期间启动。它执行与正常处理相同的服务，除了它写入的检查点在技术上是重新启动点。